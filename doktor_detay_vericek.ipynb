{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doktorlar = pd.read_csv(\"doktorlar_id.csv\")\n",
    "\n",
    "doktorlar.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 rows profile links\n",
    "for i in range(9941, 9952):\n",
    "    print(doktorlar[\"profile\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# get the details of the profile\n",
    "async def get_profile_details(url, session, i):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            doctor_details = {}\n",
    "\n",
    "            expert_profile_header = soup.find(\"div\", class_=\"expert-profile-header\")\n",
    "            doctor_details['data_id'] = expert_profile_header[\"data-id\"]\n",
    "\n",
    "            expert_point = expert_profile_header.find(\"div\", class_=\"expert-point\")\n",
    "            if expert_point:\n",
    "                doctor_details['expert_point'] = expert_point.text.strip()\n",
    "            else:\n",
    "                doctor_details['expert_point'] = None\n",
    "\n",
    "            services = expert_profile_header.find_all(\"a\", class_=\"service-list-item\")\n",
    "            doctor_details['services'] = json.dumps([service.text.strip() for service in services])\n",
    "\n",
    "            about_modal = soup.find(\"div\", id=\"aboutModal\")\n",
    "            about_content = about_modal.find(\"div\", id=\"aboutContent\")\n",
    "            doctor_details['about_content'] = about_content.text.strip()\n",
    "\n",
    "            eed_items = soup.find_all(\"div\", class_=\"eed-item\")\n",
    "            doctor_details['eed_items'] = json.dumps([item.text.strip() for item in eed_items])\n",
    "\n",
    "            ds_expert_interests = soup.find(\"div\", class_=\"ds-expert-interest\")\n",
    "            expert_interests = ds_expert_interests.find_all(\"a\")\n",
    "            doctor_details['expert_interests'] = json.dumps([interest.text.strip() for interest in expert_interests])\n",
    "\n",
    "            experience_company = soup.find(\"div\", class_=\"experience-company\")\n",
    "            if experience_company:\n",
    "                doctor_details['experience_company'] = experience_company.text.strip()\n",
    "            else:\n",
    "                doctor_details['experience_company'] = None\n",
    "\n",
    "            # get inside this tag too <script type=\"application/ld+json\">\n",
    "\n",
    "            application_ld_json = soup.find(\"script\", type=\"application/ld+json\")\n",
    "            doctor_details['application_ld_json'] = application_ld_json.text.strip()\n",
    "\n",
    "            # add the details to the doktorlar dataframe\n",
    "            doktorlar.loc[i, 'data_id'] = doctor_details['data_id']\n",
    "            doktorlar.loc[i, 'expert_point'] = doctor_details['expert_point']\n",
    "            doktorlar.loc[i, 'services'] = doctor_details['services']\n",
    "            doktorlar.loc[i, 'about_content'] = doctor_details['about_content']\n",
    "            doktorlar.loc[i, 'eed_items'] = doctor_details['eed_items']\n",
    "            doktorlar.loc[i, 'expert_interests'] = doctor_details['expert_interests']\n",
    "            doktorlar.loc[i, 'experience_company'] = doctor_details['experience_company']\n",
    "            doktorlar.loc[i, 'application_ld_json'] = doctor_details['application_ld_json']\n",
    "\n",
    "            print(f\"Profile {i} details added to the dataframe\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        error_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    tasks = []\n",
    "    for i in range(13538, len(doktorlar)):\n",
    "        url = doktorlar[\"profile\"][i]\n",
    "        tasks.append(get_profile_details(url, session, i))\n",
    "    await asyncio.gather(*tasks)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken to get the details of profiles: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the error indices by checking data_id column in the doktorlar dataframe\n",
    "error_indices = doktorlar[doktorlar[\"data_id\"].isnull()].index.tolist()\n",
    "print(len(error_indices))\n",
    "\n",
    "# call the function again for the error indices\n",
    "start_time = time.time()\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    tasks = []\n",
    "    for i in error_indices:\n",
    "        url = doktorlar[\"profile\"][i]\n",
    "        tasks.append(get_profile_details(url, session, i))\n",
    "    await asyncio.gather(*tasks)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken to get the details of profiles: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the doktorlar dataframe to a csv file\n",
    "doktorlar.to_csv(\"doktorlar_details.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doktorlar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy id, data_id, application_ld_json columns to a new dataframe\n",
    "doktorlar_data_id = doktorlar[[\"id\", \"data_id\", \"application_ld_json\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doktorlar_data_id.to_csv(\"doktorlar_data_id.csv\", index=False)\n",
    "doktorlar_data_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove application_ld_json column from the doktorlar dataframe\n",
    "#Â doktorlar.drop(columns=[\"application_ld_json\"], inplace=True)\n",
    "doktorlar.to_csv(\"doktorlar_details.csv\", index=False, encoding=\"utf-8\")\n",
    "doktorlar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix unicode characters in the dataframe and save it to a csv file\n",
    "doktorlar = pd.read_csv(\"doktorlar_details.csv\", encoding=\"utf-8\")\n",
    "doktorlar.to_csv(\"doktorlar_details.csv\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
